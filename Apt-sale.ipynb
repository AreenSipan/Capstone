{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0af8db3",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "eb780020",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-03T14:35:25.355672Z",
     "start_time": "2021-12-03T14:35:24.623875Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from fake_useragent import UserAgent\n",
    "import re\n",
    "import urllib.parse\n",
    "import urllib.request\n",
    "import random\n",
    "import shutil\n",
    "import csv\n",
    "from csv import writer\n",
    "from pathlib import Path\n",
    "from urllib.request import urlopen\n",
    "import json\n",
    "from geopy.geocoders import GoogleV3, Nominatim\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "40c1b448",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-03T14:35:33.713746Z",
     "start_time": "2021-12-03T14:35:33.691897Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ua = UserAgent()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a54f060",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "07c3d580",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-03T14:35:37.132521Z",
     "start_time": "2021-12-03T14:35:37.113809Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def flatten(A):\n",
    "    rt = []\n",
    "    for i in A:\n",
    "        if isinstance(i,list): rt.extend(flatten(i))\n",
    "        else: rt.append(i)\n",
    "    return rt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "728fe78e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-03T15:11:49.660875Z",
     "start_time": "2021-12-03T15:11:49.652888Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def append_list_as_row_list(file_name, list_of_elem):\n",
    "    with open(file_name, 'a+', newline='', encoding='utf-8-sig') as write_obj:\n",
    "        csv_writer = writer(write_obj)\n",
    "        csv_writer.writerow(list_of_elem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d59ff92",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Link Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5c316a7f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-03T14:36:03.169685Z",
     "start_time": "2021-12-03T14:36:03.164627Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36'}\n",
    "\n",
    "\n",
    "pages = set()\n",
    "def getLinks(pageUrl):\n",
    "    global pages\n",
    "    html = requests.get(pageUrl, headers = headers)\n",
    "    bsObj = BeautifulSoup(html.text, 'html.parser')\n",
    "    for link in bsObj.findAll(\"a\", href=re.compile(\"https://myrealty.am/en/apartments-for-sale/763700\\?page=\\d+$\")):\n",
    "        if 'href' in link.attrs:\n",
    "            if link.attrs['href'] not in pages:\n",
    "                newPage = link.attrs['href']\n",
    "                pages.add(newPage)\n",
    "                getLinks(newPage)\n",
    "getLinks(\"https://myrealty.am/en/apartments-for-sale/763700\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23e0a27",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Get Individual Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "93ae5fe8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-03T14:45:41.707964Z",
     "start_time": "2021-12-03T14:36:07.401499Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edbd07c415bb4b4085e4feb3481dab08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/117 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "links = []\n",
    "\n",
    "for link in tqdm(pages):\n",
    "    header = {'User-Agent':str(ua.random)}\n",
    "    content = requests.get(link, headers = header)\n",
    "    soup = BeautifulSoup(content.content, 'html.parser')\n",
    "    \n",
    "    aptrent_container = soup.find('div', {'class':'row no-gutters items-list'})\n",
    "    btf_aptrent_container = BeautifulSoup(str(aptrent_container), 'html.parser')\n",
    "    \n",
    "    links_raw = btf_aptrent_container.find_all('a', {'class':'btn btn-pink-transparent btn-cs text-uppercase item-more-btn ml-auto'})\n",
    "    links_clean = [i['href'] for i in links_raw]\n",
    "    links.append(links_clean)\n",
    "\n",
    "links = flatten(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "094e8485",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4195"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(links)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271f17f7",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Scrape Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e6bd85",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Create Empty CSV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f2c69747",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-03T15:12:18.503620Z",
     "start_time": "2021-12-03T15:12:18.472977Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "columns = ['ID', 'Location', 'Region', 'District', 'Street', 'Price', 'Price/sqm', 'Views', 'Area', 'Room', 'Floor', 'Storeys',\n",
    "           'Bathrooms', 'BuildType', 'CeilingHeight', 'Condition', 'Date Added', \n",
    "           'Date Edited', 'Date Scraped', 'Latitude', 'Longitude', 'Address',\n",
    "           'Facilities', 'Additional Info', 'url']\n",
    "\n",
    "data = pd.DataFrame(columns=columns)\n",
    "data.to_csv('apt.sale.0805.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac741982",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Scrape & Store "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e642576e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-03T15:13:16.376006Z",
     "start_time": "2021-12-03T15:12:21.905524Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd6530c7f9fb48378066baffeb184f2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4195 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for apt in tqdm(links):\n",
    "    header = {'User-Agent':str(ua.random)}\n",
    "    content = requests.get(apt, headers = header)\n",
    "    \n",
    "    soup = BeautifulSoup(content.content, 'html.parser')\n",
    "    \n",
    "    try:\n",
    "        test_id = soup.find('div', {'class':'item-view-id'})\n",
    "        if test_id:\n",
    "            ID = test_id.text.strip().split(' ')[1]\n",
    "        else:\n",
    "            ID = None\n",
    "\n",
    "        test_location = soup.find('div', {'class':'col-auto item-view-address pl-0 mb-2 mt-1'})\n",
    "        if test_location:\n",
    "            Location = test_location.text.strip()\n",
    "            Region = test_location.text.strip().split(',')[0]\n",
    "            District = test_location.text.strip().split(',')[1]\n",
    "            Street = test_location.text.strip().split(',')[2]\n",
    "        else:\n",
    "            Location = None\n",
    "            Region = None\n",
    "            District = None\n",
    "            Street = None\n",
    "\n",
    "        test_price = soup.find('div', {'class':'item-view-price'})\n",
    "        if test_price:\n",
    "            Price = test_price.text.strip().split('/')[0].replace(\",\", \"\")\n",
    "            Price_sq = test_price.text.strip().split('/')[1].replace(\"SQ. M. \", \"\").replace(\",\", \"\")\n",
    "        else:\n",
    "            Price = None\n",
    "            Price_sq = None\n",
    "\n",
    "        test_views = soup.find('span', {'class':'item-view-count'})\n",
    "        if test_views:\n",
    "            Views = test_views.text.strip()\n",
    "        else:\n",
    "            Views = None\n",
    "\n",
    "        details = [i.text for i in soup.find('div', {'class': 'item-view-price-params'}).findAll('span')]\n",
    "        if details:\n",
    "            Area = details[0].split()[0]\n",
    "            Rooms = details[1].replace(\"+\", \"\")\n",
    "            Floor = details[2].split('/')[0]\n",
    "            Storeys = details[2].split('/')[1]\n",
    "        else:\n",
    "            Area = None\n",
    "            Rooms = None\n",
    "            Floor = None\n",
    "            Storeys = None\n",
    "\n",
    "        params = [i.text for i in soup.findAll('div', {'class': 'col-5'})]\n",
    "        if params:\n",
    "            Bathrooms = params[1].split()[0].replace(\"+\", \"\")\n",
    "            BuildType = params[2].split()[0]\n",
    "            CeilingHeight = params[3].split()[0]\n",
    "            Condition = params[4].strip()\n",
    "        else:\n",
    "            Bathrooms = None\n",
    "            BuildType = None\n",
    "            CeilingHeight = None\n",
    "            Condition = None\n",
    "            \n",
    "        date_id = [i.text for i in soup.findAll('div', {'class': 'row no-gutters flex-column item-view-date'})]\n",
    "        if date_id:\n",
    "            DateAdded = str(date_id).split()[2]\n",
    "            DateEdited = str(date_id).split()[5]\n",
    "        else:\n",
    "            DateAdded = None\n",
    "            DateEdited = None\n",
    "        DateScraped = date.today().strftime(\"%d.%m.%Y\")\n",
    "        \n",
    "        \n",
    "        test_coord = soup.find(\"div\", {'id': 'yandex_map_item_view'}).attrs\n",
    "        if test_coord:\n",
    "            Latitude = test_coord['data-lat']\n",
    "            Longitude = test_coord['data-lng']\n",
    "            geolocator1 = Nominatim(user_agent=\"coordinateconverter\")\n",
    "            location1 = geolocator1.reverse(\"{}, {}\".format(Latitude, Longitude))\n",
    "            Address = location1.address\n",
    "            \n",
    "        else: \n",
    "            Latitude = None\n",
    "            Longitude = None\n",
    "            Address = None\n",
    "            \n",
    "        test_facilities = [i.text for i in soup.findAll('div', {'class': 'row item-view-facilities mb-4'})[0].findAll('label')]\n",
    "        if test_facilities:\n",
    "            Facilities = test_facilities\n",
    "        else:\n",
    "            Facilities = None\n",
    "\n",
    "        test_info = [i.text for i in soup.findAll('div', {'class': 'row item-view-facilities mb-4'})[1].findAll('label')]\n",
    "        if test_info:\n",
    "            Additional = test_info\n",
    "        else:\n",
    "            Additional = None\n",
    "            \n",
    "        url = apt\n",
    "            \n",
    "    except:\n",
    "        continue\n",
    "    content = [ID, Location, Region, District, Street, Price, Price_sq, Views, Area, Rooms, Floor, Storeys,\n",
    "               Bathrooms, BuildType, CeilingHeight, Condition, DateAdded, DateEdited, DateScraped,\n",
    "               Latitude, Longitude, Address, Facilities, Additional, url]\n",
    "    append_list_as_row_list('apt.sale.0805.csv', content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8978eab7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2830"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('apt.sale.0805.csv')\n",
    "len(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
